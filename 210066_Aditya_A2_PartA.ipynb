{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2A\n",
    "\n",
    "**Name**: Aditya Nikam                               </br>\n",
    "**Roll No.**: 210066"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import gymnasium as gym\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Represents a Stochastic Maze problem Gym Environment which provides a Fully observable\n",
    "MDP\n",
    "'''\n",
    "class StochasticMazeEnv(gym.Env):\n",
    "    '''\n",
    "    StochasticMazeEnv represents the Gym Environment for the Stochastic Maze environment\n",
    "    States : [0,1,2,3,4,5,6,7,8,9,10,11]\n",
    "    Actions : [\"Left\":0, \"Up\":1, \"Right\":2, \"Down\":3]\n",
    "    '''\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self,initial_state=0,no_states=12,no_actions=4):\n",
    "        '''\n",
    "        Constructor for the StochasticMazeEnv class\n",
    "\n",
    "        Args:\n",
    "            initial_state : starting state of the agent\n",
    "            no_states : The no. of possible states which is 12\n",
    "            no_actions : The no. of possible actions which is 4\n",
    "            \n",
    "        '''\n",
    "        self.initial_state = initial_state\n",
    "        self.stat1e = self.initial_state\n",
    "        self.nA = no_actions\n",
    "        self.nS = no_states\n",
    "        self.actions_dict = {\"L\":0, \"U\":1, \"R\":2, \"D\":3}\n",
    "        self.prob_dynamics = {\n",
    "            0: {\n",
    "                0: [(0.8, 0, -0.01, False), (0.1, 0, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                1: [(0.8, 0, -0.01, False), (0.1, 0, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "                2: [(0.8, 1, -0.01, False), (0.1, 0, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                3: [(0.8, 4, -0.01, False), (0.1, 0, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "            },\n",
    "            1: {\n",
    "                0: [(0.8, 0, -0.01, False), (0.1, 1, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "                1: [(0.8, 1, -0.01, False), (0.1, 0, -0.01, False), (0.1, 2, -0.01, False)],\n",
    "                2: [(0.8, 2, -0.01, False), (0.1, 1, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "                3: [(0.8, 1, -0.01, False), (0.1, 0, -0.01, False), (0.1, 2, -0.01, False)],\n",
    "            },\n",
    "            2: {\n",
    "                0: [(0.8, 1, -0.01, False), (0.1, 2, -0.01, False), (0.1, 6, -0.01, False)],\n",
    "                1: [(0.8, 2, -0.01, False), (0.1, 1, -0.01, False), (0.1, 3, +1, True)],\n",
    "                2: [(0.8, 3, +1, True), (0.1, 2, -0.01, False), (0.1, 6, -0.01, False)],\n",
    "                3: [(0.8, 6, -0.01, False), (0.1, 1, -0.01, False), (0.1, 3, +1, True)],\n",
    "            },\n",
    "            3: {\n",
    "                0: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "                1: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "                2: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "                3: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "            },\n",
    "            4: {\n",
    "                0: [(0.8, 4, -0.01, False), (0.1, 0, -0.01, False), (0.1, 8, -0.01, False)],\n",
    "                1: [(0.8, 0, -0.01, False), (0.1, 4, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                2: [(0.8, 4, -0.01, False), (0.1, 0, -0.01, False), (0.1, 8, -0.01, False)],\n",
    "                3: [(0.8, 8, -0.01, False), (0.1, 4, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "            },\n",
    "            6: {\n",
    "                0: [(0.8, 6, -0.01, False), (0.1, 2, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                1: [(0.8, 2, -0.01, False), (0.1, 6, -0.01, False), (0.1, 7, -1, True)],\n",
    "                2: [(0.8, 7, -1, True), (0.1, 2, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                3: [(0.8, 10, -0.01, False), (0.1, 6, -0.01, False), (0.1, 7, -1, True)],\n",
    "            },\n",
    "            7: {\n",
    "                0: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "                1: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "                2: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "                3: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "            },\n",
    "            8: {\n",
    "                0: [(0.8, 8, -0.01, False), (0.1, 8, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                1: [(0.8, 4, -0.01, False), (0.1, 8, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "                2: [(0.8, 9, -0.01, False), (0.1, 8, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                3: [(0.8, 8, -0.01, False), (0.1, 8, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "            },\n",
    "            9: {\n",
    "                0: [(0.8, 8, -0.01, False), (0.1, 9, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "                1: [(0.8, 9, -0.01, False), (0.1, 8, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                2: [(0.8, 10, -0.01, False), (0.1, 9, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "                3: [(0.8, 9, -0.01, False), (0.1, 8, -0.01, False), (0.1, 10, -0.01, False)]\n",
    "            },\n",
    "            10: {\n",
    "                0: [(0.8, 9, -0.01, False), (0.1, 6, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                1: [(0.8, 6, -0.01, False), (0.1, 9, -0.01, False), (0.1, 11, -0.01, False)],\n",
    "                2: [(0.8, 11, -0.01, False), (0.1, 6, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                3: [(0.8, 10, -0.01, False), (0.1, 9, -0.01, False), (0.1, 11, -0.01, False)]\n",
    "            },\n",
    "            11: {\n",
    "                0: [(0.8, 10, -0.01, False), (0.1, 7, -1, True), (0.1, 11, -0.01, False)],\n",
    "                1: [(0.8, 7, -1, True), (0.1, 10, -0.01, False), (0.1, 11, -0.01, False)],\n",
    "                2: [(0.8, 11, -0.01, False), (0.1, 7, -1, True), (0.1, 11, -0.01, False)],\n",
    "                3: [(0.8, 11, -0.01, False), (0.1, 11, -0.01, False), (0.1, 10, -0.01, False)]\n",
    "            }\n",
    "        }\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        '''\n",
    "        Resets the environment\n",
    "        Returns:\n",
    "            observations containing player's current state\n",
    "        '''\n",
    "        self.state = self.initial_state\n",
    "        return self.get_obs()\n",
    "\n",
    "    def get_obs(self):\n",
    "        '''\n",
    "        Returns the player's state as the observation of the environment\n",
    "        '''\n",
    "        return (self.state)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        '''\n",
    "        Renders the environment\n",
    "        '''\n",
    "        print(\"Current state: {}\".format(self.state))\n",
    "\n",
    "    def sample_action(self):\n",
    "        '''\n",
    "        Samples and returns a random action from the action space\n",
    "        '''\n",
    "        return random.randint(0, self.nA)\n",
    "    def P(self):\n",
    "        '''\n",
    "        Defines and returns the probabilty transition matrix which is in the form of a nested dictionary\n",
    "        '''\n",
    "        self.prob_dynamics = {\n",
    "            0: {\n",
    "                0: [(0.8, 0, -0.01, False), (0.1, 0, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                1: [(0.8, 0, -0.01, False), (0.1, 0, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "                2: [(0.8, 1, -0.01, False), (0.1, 0, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                3: [(0.8, 4, -0.01, False), (0.1, 0, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "            },\n",
    "            1: {\n",
    "                0: [(0.8, 0, -0.01, False), (0.1, 1, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "                1: [(0.8, 1, -0.01, False), (0.1, 0, -0.01, False), (0.1, 2, -0.01, False)],\n",
    "                2: [(0.8, 2, -0.01, False), (0.1, 1, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "                3: [(0.8, 1, -0.01, False), (0.1, 0, -0.01, False), (0.1, 2, -0.01, False)],\n",
    "            },\n",
    "            2: {\n",
    "                0: [(0.8, 1, -0.01, False), (0.1, 2, -0.01, False), (0.1, 6, -0.01, False)],\n",
    "                1: [(0.8, 2, -0.01, False), (0.1, 1, -0.01, False), (0.1, 3, +1, True)],\n",
    "                2: [(0.8, 3, +1, True), (0.1, 2, -0.01, False), (0.1, 6, -0.01, False)],\n",
    "                3: [(0.8, 6, -0.01, False), (0.1, 1, -0.01, False), (0.1, 3, +1, True)],\n",
    "            },\n",
    "            3: {\n",
    "                0: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "                1: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "                2: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "                3: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "            },\n",
    "            4: {\n",
    "                0: [(0.8, 4, -0.01, False), (0.1, 0, -0.01, False), (0.1, 8, -0.01, False)],\n",
    "                1: [(0.8, 0, -0.01, False), (0.1, 4, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                2: [(0.8, 4, -0.01, False), (0.1, 0, -0.01, False), (0.1, 8, -0.01, False)],\n",
    "                3: [(0.8, 8, -0.01, False), (0.1, 4, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "            },\n",
    "            6: {\n",
    "                0: [(0.8, 6, -0.01, False), (0.1, 2, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                1: [(0.8, 2, -0.01, False), (0.1, 6, -0.01, False), (0.1, 7, -1, True)],\n",
    "                2: [(0.8, 7, -1, True), (0.1, 2, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                3: [(0.8, 10, -0.01, False), (0.1, 6, -0.01, False), (0.1, 7, -1, True)],\n",
    "            },\n",
    "            7: {\n",
    "                0: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "                1: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "                2: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "                3: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "            },\n",
    "            8: {\n",
    "                0: [(0.8, 8, -0.01, False), (0.1, 8, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                1: [(0.8, 4, -0.01, False), (0.1, 8, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "                2: [(0.8, 9, -0.01, False), (0.1, 8, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                3: [(0.8, 8, -0.01, False), (0.1, 8, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "            },\n",
    "            9: {\n",
    "                0: [(0.8, 8, -0.01, False), (0.1, 9, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "                1: [(0.8, 9, -0.01, False), (0.1, 8, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                2: [(0.8, 10, -0.01, False), (0.1, 9, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "                3: [(0.8, 9, -0.01, False), (0.1, 8, -0.01, False), (0.1, 10, -0.01, False)]\n",
    "            },\n",
    "            10: {\n",
    "                0: [(0.8, 9, -0.01, False), (0.1, 6, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                1: [(0.8, 6, -0.01, False), (0.1, 9, -0.01, False), (0.1, 11, -0.01, False)],\n",
    "                2: [(0.8, 11, -0.01, False), (0.1, 6, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                3: [(0.8, 10, -0.01, False), (0.1, 9, -0.01, False), (0.1, 11, -0.01, False)]\n",
    "            },\n",
    "            11: {\n",
    "                0: [(0.8, 10, -0.01, False), (0.1, 7, -1, True), (0.1, 11, -0.01, False)],\n",
    "                1: [(0.8, 7, -1, True), (0.1, 10, -0.01, False), (0.1, 11, -0.01, False)],\n",
    "                2: [(0.8, 11, -0.01, False), (0.1, 7, -1, True), (0.1, 11, -0.01, False)],\n",
    "                3: [(0.8, 11, -0.01, False), (0.1, 11, -0.01, False), (0.1, 10, -0.01, False)]\n",
    "            }\n",
    "        }\n",
    "        return self.prob_dynamics\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        '''\n",
    "        Performs the given action\n",
    "        Args:\n",
    "            action : action from the action_space to be taking in the environment\n",
    "        Returns:\n",
    "            observation - returns current state\n",
    "            reward - reward obtained after taking the given action\n",
    "            done - True if the episode is complete else False\n",
    "        '''\n",
    "        if action >= self.nA:\n",
    "            action = self.nA-1\n",
    "\n",
    "        index = np.random.choice(3,1,p=[0.8,0.1,0.1])[0]\n",
    "\n",
    "        dynamics_tuple = self.prob_dynamics[self.state][action][index]\n",
    "        self.state = dynamics_tuple[1]\n",
    "        \n",
    "\n",
    "        return self.state, dynamics_tuple[2], dynamics_tuple[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StochasticMazeEnv()\n",
    "env.reset()\n",
    "num_states = env.nS\n",
    "num_actions = env.nA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Cases for checking the environment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Random Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action\t State\t Reward\t is_Terminal\n",
      "   2 \t   1 \t -0.01 \t   False\n",
      "   2 \t   2 \t -0.01 \t   False\n",
      "   1 \t   2 \t -0.01 \t   False\n",
      "   0 \t   1 \t -0.01 \t   False\n",
      "   0 \t   0 \t -0.01 \t   False\n",
      "   1 \t   0 \t -0.01 \t   False\n",
      "   3 \t   4 \t -0.01 \t   False\n",
      "   3 \t   8 \t -0.01 \t   False\n",
      "   1 \t   4 \t -0.01 \t   False\n",
      "   0 \t   8 \t -0.01 \t   False\n",
      "   1 \t   4 \t -0.01 \t   False\n",
      "   3 \t   4 \t -0.01 \t   False\n",
      "   0 \t   0 \t -0.01 \t   False\n",
      "   1 \t   0 \t -0.01 \t   False\n",
      "   3 \t   4 \t -0.01 \t   False\n",
      "   3 \t   8 \t -0.01 \t   False\n",
      "   3 \t   8 \t -0.01 \t   False\n",
      "   2 \t   8 \t -0.01 \t   False\n",
      "   3 \t   8 \t -0.01 \t   False\n",
      "   1 \t   4 \t -0.01 \t   False\n",
      "   1 \t   0 \t -0.01 \t   False\n",
      "   0 \t   0 \t -0.01 \t   False\n",
      "   1 \t   0 \t -0.01 \t   False\n",
      "   2 \t   1 \t -0.01 \t   False\n",
      "   0 \t   1 \t -0.01 \t   False\n",
      "   3 \t   1 \t -0.01 \t   False\n",
      "   0 \t   0 \t -0.01 \t   False\n",
      "   1 \t   0 \t -0.01 \t   False\n",
      "   1 \t   0 \t -0.01 \t   False\n",
      "   0 \t   0 \t -0.01 \t   False\n",
      "   1 \t   0 \t -0.01 \t   False\n",
      "   0 \t   0 \t -0.01 \t   False\n",
      "   3 \t   4 \t -0.01 \t   False\n",
      "   1 \t   0 \t -0.01 \t   False\n",
      "   0 \t   4 \t -0.01 \t   False\n",
      "   3 \t   8 \t -0.01 \t   False\n",
      "   1 \t   4 \t -0.01 \t   False\n",
      "   1 \t   0 \t -0.01 \t   False\n",
      "   3 \t   4 \t -0.01 \t   False\n",
      "   3 \t   8 \t -0.01 \t   False\n",
      "   1 \t   4 \t -0.01 \t   False\n",
      "   0 \t   4 \t -0.01 \t   False\n",
      "   2 \t   4 \t -0.01 \t   False\n",
      "   3 \t   8 \t -0.01 \t   False\n",
      "   1 \t   4 \t -0.01 \t   False\n",
      "   2 \t   4 \t -0.01 \t   False\n",
      "   2 \t   4 \t -0.01 \t   False\n",
      "   2 \t   4 \t -0.01 \t   False\n",
      "   2 \t   4 \t -0.01 \t   False\n",
      "   1 \t   0 \t -0.01 \t   False\n",
      "   0 \t   0 \t -0.01 \t   False\n",
      "   0 \t   0 \t -0.01 \t   False\n",
      "   1 \t   0 \t -0.01 \t   False\n",
      "   2 \t   1 \t -0.01 \t   False\n",
      "   2 \t   2 \t -0.01 \t   False\n",
      "   1 \t   2 \t -0.01 \t   False\n",
      "   0 \t   1 \t -0.01 \t   False\n",
      "   2 \t   2 \t -0.01 \t   False\n",
      "   2 \t   3 \t 1 \t   True\n",
      "Total Number of steps to Reach Terminal: 59\n",
      "Final Reward: 0.4199999999999997\n"
     ]
    }
   ],
   "source": [
    "is_Terminal = False\n",
    "env.reset()\n",
    "count = 0\n",
    "total_reward = 0\n",
    "\n",
    "print(\"Action\\t\" , \"State\\t\" , \"Reward\\t\" , \"is_Terminal\")\n",
    "\n",
    "while not is_Terminal:\n",
    "    count += 1\n",
    "\n",
    "    rand_action = np.random.choice(4,1)[0]  #0 -> LEFT, 1 -> UP, 2 -> RIGHT, 3 -> DOWN\n",
    "    state, reward, is_Terminal = env.step(rand_action)\n",
    "    \n",
    "    total_reward += reward\n",
    "\n",
    "    print(\"  \", rand_action, \"\\t  \", state, \"\\t\", reward, \"\\t  \", is_Terminal)\n",
    "    \n",
    "print(\"Total Number of steps to Reach Terminal:\", count)\n",
    "print(\"Final Reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The random policy takes large number of steps to reach some terminal state which should be much higher than the number of the steps taken by a all 'Right' policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Right Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action\t State\t Reward\t is_Terminal\n",
      "   2 \t   1 \t -0.01 \t   False\n",
      "   2 \t   1 \t -0.01 \t   False\n",
      "   2 \t   2 \t -0.01 \t   False\n",
      "   2 \t   3 \t 1 \t   True\n",
      "Total Number of steps to Reach Terminal: 4\n",
      "Final Reward: 0.97\n"
     ]
    }
   ],
   "source": [
    "is_Terminal = False\n",
    "env.reset()\n",
    "count = 0\n",
    "total_reward = 0\n",
    "\n",
    "print(\"Action\\t\" , \"State\\t\" , \"Reward\\t\" , \"is_Terminal\")\n",
    "\n",
    "while not is_Terminal:\n",
    "    count += 1\n",
    "\n",
    "    right_action = 2  #0 -> LEFT, 1 -> UP, 2 -> RIGHT, 3 -> DOWN\n",
    "    state, reward, is_Terminal = env.step(right_action)\n",
    "    \n",
    "    total_reward += reward\n",
    "\n",
    "    print(\"  \", right_action, \"\\t  \", state, \"\\t\", reward, \"\\t  \", is_Terminal)\n",
    "    \n",
    "print(\"Total Number of steps to Reach Terminal:\", count)\n",
    "print(\"Final Reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The right policy most of the time reaches the goal state in just 3 steps which is expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Find an optimal policy to navigate the given environment using Policy Iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This function takes input P(taking action a/ given state s), P(going to state s1/ given current state s), \n",
    "value function and expected reward being in state s and taken action a (R)'''\n",
    "def policy_evaluation(P_a_s, P_s1_s, value, R):    \n",
    "    d_f = 0.9\n",
    "    is_not_same = True\n",
    "    k = 0\n",
    "    R_pi = np.zeros(12)\n",
    "    for s in range(0,12):\n",
    "        R_pi[s] = np.dot(R[s,:],P_a_s[s])\n",
    "    delta = 0\n",
    "    v_k = np.zeros(12)\n",
    "    while delta < 0.000001:\n",
    "        for s in range(0,12):\n",
    "            v_k[s] = R_pi[s] + d_f*np.dot(P_s1_s[s,:],value)\n",
    "            delta = max(delta,np.abs(v_k[s]-value[s]))\n",
    "            value[s] = v_k[s]\n",
    "        k = k + 1\n",
    "    return v_k, R_pi\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''THis function takes input P(going to state s1/ given current state s and taken action a), value function,\n",
    "P(next state s1 / current state s) and expected reward being in state s and taken action a (R)'''\n",
    "def policy_improvement(P_s1_sa, v_k, P_s1_s, R):\n",
    "    discount_factor = 0.9\n",
    "    q_k = np.zeros((12,4))\n",
    "    for s in range(0,12):\n",
    "        for a in range(0,4):\n",
    "            q_k[s,a] = R[s,a] + discount_factor*np.dot(P_s1_sa[s,a,:],v_k)\n",
    "            \n",
    "    P_a_s = np.zeros((12,4))    \n",
    "    index = np.argmax(q_k,axis=1)\n",
    "    for s in range(0,12):\n",
    "        P_a_s[s,index[s]] = 1\n",
    "        \n",
    "    for s1 in range(0,12):\n",
    "        for s in range(0,12):\n",
    "            P_s1_s[s][s1] = np.dot((P_s1_sa[s,:,s1]),(P_a_s[s]))\n",
    "\n",
    "    return q_k, P_a_s, P_s1_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This function takes input P(next state s1 / given current state s), R and \n",
    "P(going to state s1/ given current state s and taken action a)'''\n",
    "def policy_iteration(R, P_s1_sa):\n",
    "    value = np.zeros(12)\n",
    "    v_k1 = value\n",
    "    P_a_s = [0.25, 0.25, 0.25, 0.25]*np.ones((12,4)) # randomly initiating policy\n",
    "    P_s1_s = np.zeros((12,12))\n",
    "    for s1 in range(0,12):\n",
    "        for s in range(0,12):\n",
    "            P_s1_s[s][s1] = np.dot((P_s1_sa[s,:,s1]),(P_a_s[s]))\n",
    "    P_a_s1 = P_a_s\n",
    "    P_s1_sa1 = P_s1_sa\n",
    "    P_s1_s1 = P_s1_s\n",
    "    optimal = False\n",
    "    while not optimal:\n",
    "        v_k, R_pi = policy_evaluation(P_a_s1, P_s1_s1, v_k1, R)\n",
    "        q_k, P_a_s, P_s1_s = policy_improvement(P_s1_sa1, v_k, P_s1_s1, R)\n",
    "        action = np.argmax(P_a_s, axis=1)\n",
    "        # print(action)\n",
    "        if np.array_equal(P_a_s1, P_a_s):\n",
    "            optimal = True\n",
    "        v_k1 = v_k\n",
    "        P_a_s1 = P_a_s\n",
    "        P_s1_sa1 = P_s1_sa\n",
    "\n",
    "        \n",
    "    return P_a_s, v_k, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability of going to state s1 given current state s and action a\n",
    "P = env.prob_dynamics\n",
    "P_s1_sa = np.zeros((12,4,12))  # current state, action, next state\n",
    "\n",
    "for s in range(0,12):\n",
    "    for a in range(0,4):\n",
    "        if s == 5:\n",
    "            break\n",
    "        tuple = P[s][a]\n",
    "        sums_dict = {}\n",
    "        \n",
    "        # Calculate the sums\n",
    "        for elem in tuple:\n",
    "            second_element = elem[1]\n",
    "            if second_element not in sums_dict:\n",
    "                sums_dict[second_element] = 0\n",
    "                P_s1_sa[s][a][second_element] = 0\n",
    "                \n",
    "            sums_dict[second_element] += elem[0]\n",
    "            P_s1_sa[s][a][second_element] += elem[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability of getting reward r given current state s and action a\n",
    "P_r_sa = np.zeros((12,4,3))  # current state, action, reward\n",
    "for s in range(0,12):\n",
    "    for a in range(0,4):\n",
    "        if s == 5:\n",
    "            break\n",
    "        tuple = P[s][a]\n",
    "        sums_dict = {}\n",
    "        \n",
    "        # Calculate the sums\n",
    "        for elem in tuple:\n",
    "            third_element = elem[2]\n",
    "            if elem[2] == -0.01:\n",
    "                r = 0\n",
    "            if elem[2] == -1:\n",
    "                r = 1\n",
    "            if elem[2] == 1:\n",
    "                r = 2\n",
    "            if third_element not in sums_dict:\n",
    "                sums_dict[third_element] = 0\n",
    "                P_r_sa[s][a][r] = 0\n",
    "                \n",
    "            sums_dict[third_element] += elem[0]\n",
    "            P_r_sa[s][a][r] += elem[0]\n",
    "\n",
    "# print(P_r_sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expected reward being in state s and taken action a (R) \n",
    "r = np.array([-0.01, -1, 1])\n",
    "R = np.zeros((12,4))\n",
    "# R[5,:] = -0.01*np.ones(4)\n",
    "Rr = {}\n",
    "for s in range(0,12):\n",
    "    if s == 5:\n",
    "        continue\n",
    "    for a in range(0,4):\n",
    "        R[s][a] = np.dot(r,P_r_sa[s][a])        \n",
    "# print(R)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 0 1 0 1 0 2 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "P_A_S, v_k, action =  policy_iteration(R, P_s1_sa)\n",
    "# print(P_A_S)\n",
    "# print(v_k)\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Find an optimal policy to navigate the given environment using Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This function takes input P(taking action a/ given state s), P(going to state s1/ given current state s), \n",
    "value function and expected reward being in state s and taken action a (R)'''\n",
    "def value_evaluation(P_a_s, P_s1_s, value, R):\n",
    "    \n",
    "    discount_factor = 0.9\n",
    "    is_not_same = True\n",
    "    k = 0\n",
    "    R_pi = np.zeros(12)\n",
    "    for s in range(0,12):\n",
    "        R_pi[s] = np.dot(R[s,:],P_a_s[s])\n",
    "\n",
    "    for s in range(0,12):\n",
    "        v_k[s] = np.max(R[s,a] + discount_factor*np.dot(P_s1_sa[s,a,:],value))\n",
    "        value[s] = v_k[s]\n",
    "    value = v_k\n",
    "    return v_k, R_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This function takes input P(going to state s1/ given current state s and taken action a), value function\n",
    "and expected reward being in state s and taken action a (R)'''\n",
    "def policy_improvement(P_s1_sa, v_k, P_s1_s, R):\n",
    "    discount_factor = 0.9\n",
    "    q_k = np.zeros((12,4))\n",
    "    for s in range(0,12):\n",
    "        for a in range(0,4):\n",
    "            q_k[s,a] = R[s,a] + discount_factor*np.dot(P_s1_sa[s,a,:],v_k)\n",
    "            \n",
    "    # print(q_k)\n",
    "    P_a_s = np.zeros((12,4))\n",
    "    \n",
    "    index = np.argmax(q_k,axis=1)\n",
    "    for s in range(0,12):\n",
    "        P_a_s[s,index[s]] = 1\n",
    "    \n",
    "    \n",
    "    for s1 in range(0,12):\n",
    "        for s in range(0,12):\n",
    "            P_s1_s[s][s1] = np.dot((P_s1_sa[s,:,s1]),(P_a_s[s]))\n",
    "\n",
    "    return q_k, P_a_s, P_s1_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''THis function takes input P(going to state s1/ given current state s), expected reward being\n",
    "in state s and taken action a (R) and P(going to state s1/ given current state s and taken action a)'''\n",
    "def value_iteration(R, P_s1_sa):\n",
    "    value = np.zeros(12)\n",
    "    v_k1 = value\n",
    "    P_a_s = [0.25, 0.25, 0.25, 0.25]*np.ones((12,4))\n",
    "    P_s1_s = np.zeros((12,12))\n",
    "    for s1 in range(0,12):\n",
    "        for s in range(0,12):\n",
    "            P_s1_s[s][s1] = np.dot((P_s1_sa[s,:,s1]),(P_a_s[s]))\n",
    "    P_a_s1 = P_a_s\n",
    "    P_s1_sa1 = P_s1_sa\n",
    "    P_s1_s1 = P_s1_s\n",
    "    optimal = False\n",
    "    while not optimal:\n",
    "        v_k, R_pi = value_evaluation(P_a_s1, P_s1_s1, v_k1, R)\n",
    "        q_k, P_a_s, P_s1_s = policy_improvement(P_s1_sa1, v_k, P_s1_s, R)\n",
    "        action = np.argmax(P_a_s, axis=1)\n",
    "        if np.array_equal(P_a_s1, P_a_s):\n",
    "            optimal = True\n",
    "        v_k1 = v_k\n",
    "        P_a_s1 = P_a_s\n",
    "        P_s1_sa1 = P_s1_sa\n",
    "    return P_a_s, v_k, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability of going to state s1 given current state s and action a\n",
    "P_s1_sa = np.zeros((12,4,12))  # current state, action, next state\n",
    "\n",
    "for s in range(0,12):\n",
    "    for a in range(0,4):\n",
    "        if s == 5:\n",
    "            break\n",
    "        tuple = P[s][a]\n",
    "        sums_dict = {}\n",
    "        \n",
    "        # Calculate the sums\n",
    "        for elem in tuple:\n",
    "            second_element = elem[1]\n",
    "            if second_element not in sums_dict:\n",
    "                sums_dict[second_element] = 0\n",
    "                P_s1_sa[s][a][second_element] = 0\n",
    "                \n",
    "            sums_dict[second_element] += elem[0]\n",
    "            P_s1_sa[s][a][second_element] += elem[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability of reward given current state s and action a\n",
    "P_r_sa = np.zeros((12,4,3))  # current state, action, reward\n",
    "for s in range(0,12):\n",
    "    for a in range(0,4):\n",
    "        if s == 5:\n",
    "            break\n",
    "        tuple = P[s][a]\n",
    "        sums_dict = {}\n",
    "        \n",
    "        # Calculate the sums\n",
    "        for elem in tuple:\n",
    "            third_element = elem[2]\n",
    "            if elem[2] == -0.01:\n",
    "                r = 0\n",
    "            if elem[2] == -1:\n",
    "                r = 1\n",
    "            if elem[2] == 1:\n",
    "                r = 2\n",
    "            if third_element not in sums_dict:\n",
    "                sums_dict[third_element] = 0\n",
    "                P_r_sa[s][a][r] = 0\n",
    "                \n",
    "            sums_dict[third_element] += elem[0]\n",
    "            P_r_sa[s][a][r] += elem[0]\n",
    "\n",
    "# print(P_r_sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expected reward being in state s and taken action a (R) \n",
    "r = np.array([-0.01, -1, 1])\n",
    "R = np.zeros((12,4))\n",
    "# R[5,:] = -0.01*np.ones(4)\n",
    "Rr = {}\n",
    "for s in range(0,12):\n",
    "    if s == 5:\n",
    "        continue\n",
    "    for a in range(0,4):\n",
    "        R[s][a] = np.dot(r,P_r_sa[s][a])        \n",
    "# print(R)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03296746 -0.01180231  0.10744032  3.439      -0.03454841  0.\n",
      " -0.40238685 -3.439      -0.03473848 -0.03674351 -0.0371464  -0.03721398]\n",
      "[2 2 2 0 1 0 0 0 0 0 3 3]\n"
     ]
    }
   ],
   "source": [
    "P_A_S, v_k, action =  value_iteration(R, P_s1_sa)\n",
    "# print(P_A_S)\n",
    "print(v_k)\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Compare PI and VI in terms of convergence. Is the policy obtained by both same?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "4cafede8658e71bdc4b7180bcd658951c639327337cbd78715b7c29dc66075fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
